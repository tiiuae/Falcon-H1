## üöÄ Introduction

We are excited to introduce **Falcon-H1**, the latest evolution in the Falcon family of large language models. Built upon an advanced **hybrid architecture**‚Äîwhere each block integrates both **State Space Models (SSMs)** and **Attention Mechanisms**, with all channels passing through both and their outputs are finally **summed**.

These models span a wide range of scales, from **500 million to 34 billion parameters**, making them suitable for both lightweight inference on edge devices and large-scale deployments in data centers.

**Falcon-H1** was initially trained with support for **18 core languages**, extended to cover **100+ languages**, achieving state-of-the-art multilingual and reasoning performances in **instruction following**, **maths**, **coding**, and **conversational tasks**.

## ‚ú® Key Highlights

- **Hybrid Architecture**: Each transformer block processes all channels through both **SSM** and **Attention** in parallel, then **sums the outputs**. This allows the model to benefit from both **long-range memory** (via SSMs) and **local/global attention** simultaneously.

- **Scalable Sizes**: Models available at multiple scales: **500M**, **1.3B**, **7B**, **14B**, and **34B** parameters.

- **Multilingual by Design**: Native training in **18 languages**, with scalability to 100+ languages thanks to our **multilingual
tokenizer** trained on diverse language datasets, with strong **zero-shot translation** and **instruction-following** abilities.

- **Efficient Reasoning**: The hybrid structure enhances **reasoning** and **task generalization**.

- **Instruction-Following and Agent Capabilities**: Tuned for **instruction following**, **multi-turn conversations**, and already **integrated with major inference engines** such as **vLLM**, **Hugging Face Transformers**, and **llama.cpp** ‚Äî with more coming soon.

## ü¶Ö Falcon-H1

Built by the **Technology Innovation Institute (TII)** in Abu Dhabi, **Falcon-H1** is the latest step in pushing the frontier of hybrid transformer design. This open-source LLM line combines the power of **Attention** with **State Space Models (SSMs)** inside each block‚Äîno routing tricks, just pure, parallel architectural synergy.

Access Falcon-H1 across platforms and resources:

- üß† [Falcon-H1 Chat Interface](#)
- ü§ó [Hugging Face Hub](#)
- üìÑ [Technical Paper](#)
- üì∞ [Blog Post](#)
- üìö [Documentation](#)
- üñ•Ô∏è [Live Demo](#)
- üåê [Discord Community](#)

---

## üí° What is Falcon-H1?

Falcon-H1 is a **hybrid language model series** ranging from **500M** to **34B parameters**, purpose-built for **multilingual tasks**, **instruction following**, and **tool-enhanced agents**. Each block in the architecture processes data through **both attention and SSM pathways**, with outputs merged for richer context retention and more robust generalization.

Falcon-H1 natively supports **18 languages**, and has been extended to **100+**, enabling it to operate effectively in both zero-shot and few-shot multilingual environments.

---

## üß≠ Where to Start?

You can explore the following resources to begin working with Falcon-H1:

- **Quick Deploy**: Try Falcon-H1 instantly using our hosted [Chat Interface](#) or the [Live Demo](#).
- **Inference Toolkits**: Compatible out-of-the-box with **vLLM**, **Transformers**, and **llama.cpp**. Other runtimes are in progress.
- **Local Setup**: Full **GGUF** and **HF** formats available. Run it efficiently on both GPU and CPU.
- **Docs and Examples**: Dive into tutorials, quantization steps, training tips, and integration guides via the [Documentation Portal](#).
- **Research**: Learn more about our novel hybrid design in the [Falcon-H1 paper](#), or read insights from our [engineering blog](#).

---

## üì¶ Falcon-H1 Features at a Glance

- **Parallel Hybrid Blocks**: Attention + SSM in every layer.
- **100+ Languages Supported**: Multilingual instruction, chat, and translation.
- **Full Ecosystem Integration**: Runs on widely used inference stacks and supports common file formats (**HF**, **GGUF**).
- **Scalable Sizes**: From **500M** to **34B**.
- **Quantized + Fine-tune Friendly**: Models available in **8-bit**, **4-bit**, and standard **FP16**.

---

## üß† Join the Community

Got feedback or want to build with Falcon-H1?  
Join the conversation on [Discord](#), follow us on [Hugging Face](#), visit our [official website](https://falconllm.tii.ae/), or check out our roadmap and open issues on GitHub.

---
