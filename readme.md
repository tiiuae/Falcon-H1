ðŸš€ Introduction
We are excited to introduce Falcon-H, the latest evolution in the Falcon family of large language models. Built upon an advanced hybrid architectureâ€”where each block integrates both State Space Models (SSMs) and Attention Mechanisms, with all channels passing through both and their outputs summedâ€”Falcon-H sets a new benchmark in neural representation and reasoning capability.

These models span a wide range of scales, from 500 million to 34 billion parameters, making them suitable for both lightweight inference on edge devices and large-scale deployments in data centers.

Falcon-H was initially trained with support for 18 core languages, extended to cover 100+ languages and dialects, achieving state-of-the-art multilingual and reasoning performances in instruction following, maths, coding and conversational tasks.
